{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3832f50b-d927-4bbd-a6a5-700cab989e59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odd: 496\neven: 514\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "import pyspark\n",
    "spark=SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "RDD = spark.sparkContext.textFile(\"/FileStore/tables/integer.txt\")\n",
    "RDD=RDD.map(lambda x:int(x)%2)\n",
    "RDD=RDD.map(lambda word: (word, 1))\n",
    "list=RDD.reduceByKey(add).collect()\n",
    "print(\"odd:\", list[1][1])\n",
    "print(\"even:\", list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79dcf611-71b0-4106-a04f-bc579c9acc2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sales', 3488491), ('Research', 3328284), ('Developer', 3221394), ('QA', 3360624), ('Marketing', 3158450)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "import pyspark\n",
    "spark=SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "RDD = spark.sparkContext.textFile(\"/FileStore/tables/salary.txt\")\n",
    "RDD=RDD.map(lambda line: (line.split(\" \")[0],int(line.split(\" \")[1])))\n",
    "list=RDD.reduceByKey(add).collect()\n",
    "print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c15c695-67be-4293-94f3-7a4140ddfee5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shakespeare', 22)\n('GUTENBERG', 99)\n('WILLIAM', 115)\n('WORLD', 98)\n('COLLEGE', 98)\n('why', 91)\n('Lord', 341)\n('Library', 2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "import pyspark\n",
    "spark=SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "RDD = spark.sparkContext.textFile(\"/FileStore/tables/shakespeare_1.txt\")\n",
    "RDD=RDD.flatMap(lambda line: line.split(\" \"))\n",
    "RDD=RDD.map(lambda word: (word, 1))\n",
    "RDD=RDD.reduceByKey(lambda a, b: a + b)\n",
    "for i in RDD.collect():\n",
    "    if i[0] in [\"Shakespeare\", \"why\", \"Lord\", \"Library\", \"GUTENBERG\", \"WILLIAM\", \"COLLEGE\", \"WORLD\"]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c280ca76-408b-4335-800b-76da202d07ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 words are:\n[('the', 11397), ('and', 8777), ('I', 8556), ('of', 7873), ('to', 7421), ('a', 5672), ('my', 4913), ('in', 4600), ('you', 4060), ('And', 3547)]\nThe bottom 10 words are:\n[('utterance,', 1), ('ruffle', 1), ('conspirators.', 1), ('loves?', 1), ('seventy-five', 1), ('drachmas.', 1), ('orchards,', 1), ('Tiber;', 1), ('Mischief,', 1), ('Octavius.', 1)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "import pyspark\n",
    "spark=SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "RDD = spark.sparkContext.textFile(\"/FileStore/tables/shakespeare_1.txt\")\n",
    "RDD=RDD.flatMap(lambda line: line.split(\" \"))\n",
    "RDD=RDD.map(lambda word: (word, 1))\n",
    "RDD=RDD.reduceByKey(lambda a, b: a + b)\n",
    "RDD=RDD.sortBy(lambda x: x[1],ascending=False).collect()\n",
    "print(\"The top 10 words are:\")\n",
    "print(RDD[1:11])#Filter out empty line\n",
    "print(\"The bottom 10 words are:\")\n",
    "print(RDD[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "942eed8d-e887-47ba-9d3d-dbe6faafcc1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 rated movies are:\n[('32', 2.9166666666666665), ('90', 2.8125), ('30', 2.5), ('94', 2.473684210526316), ('23', 2.466666666666667), ('49', 2.4375), ('29', 2.4), ('18', 2.4), ('52', 2.357142857142857), ('53', 2.25), ('62', 2.25), ('92', 2.2142857142857144), ('46', 2.2), ('68', 2.1578947368421053), ('87', 2.1333333333333333), ('2', 2.1052631578947367), ('69', 2.076923076923077), ('27', 2.066666666666667), ('88', 2.0555555555555554), ('22', 2.05)]\nThe top 15 rateing users are:\n[('11', 2.2857142857142856), ('26', 2.204081632653061), ('22', 2.1607142857142856), ('23', 2.1346153846153846), ('2', 2.0652173913043477), ('17', 1.9565217391304348), ('8', 1.8979591836734695), ('24', 1.8846153846153846), ('12', 1.8545454545454545), ('3', 1.8333333333333333), ('29', 1.826086956521739), ('28', 1.82), ('9', 1.7924528301886793), ('14', 1.7894736842105263), ('16', 1.7777777777777777)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "import pyspark\n",
    "spark=SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "RDD = spark.sparkContext.textFile(\"/FileStore/tables/movies.csv\")\n",
    "RDD=RDD.filter(lambda x:x!=\"movieId,rating,userId\")\n",
    "RDD=RDD.map(lambda line: (line.split(\",\")[0],int(line.split(\",\")[1]),line.split(\",\")[2]))\n",
    "\n",
    "#count movies\n",
    "movie=RDD.map(lambda word: (word[0], 1))\n",
    "movie=movie.reduceByKey(lambda a, b: a + b)\n",
    "#count total reviews\n",
    "movie_r=RDD.map(lambda word: (word[0], word[1]))\n",
    "movie_r=movie_r.reduceByKey(lambda a, b: a + b)\n",
    "movie=movie.join(movie_r)\n",
    "#compute avg rating\n",
    "movie=movie.map(lambda word: (word[0], word[1][1]/word[1][0]))\n",
    "movie=movie.sortBy(lambda x: x[1],ascending=False).collect()\n",
    "print(\"The top 20 rated movies are:\")\n",
    "print(movie[0:20])\n",
    "\n",
    "#count users\n",
    "users=RDD.map(lambda word: (word[2], 1))\n",
    "users=users.reduceByKey(lambda a, b: a + b)\n",
    "#count total reviews\n",
    "users_r=RDD.map(lambda word: (word[2], word[1]))\n",
    "users_r=users_r.reduceByKey(lambda a, b: a + b)\n",
    "users=users.join(users_r)\n",
    "#compute avg rating\n",
    "users=users.map(lambda word: (word[0], word[1][1]/word[1][0]))\n",
    "users=users.sortBy(lambda x: x[1],ascending=False).collect()\n",
    "print(\"The top 15 rateing users are:\")\n",
    "print(users[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beceb4f6-942a-4326-9a17-a56eda5101c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 1.1814695827849822\nMSE= 1.3958703750461199\nMAE= 0.7863305180672572\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator,TrainValidationSplit\n",
    "spark=SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "RDD = spark.sparkContext.textFile(\"/FileStore/tables/movies.csv\").filter(lambda x:x!=\"movieId,rating,userId\")\n",
    "df=RDD.map(lambda line: (int(line.split(\",\")[0]),int(line.split(\",\")[1]),int(line.split(\",\")[2]))).toDF()\n",
    "train, test = df.randomSplit(weights=[0.7, 0.3],seed=2)\n",
    "als=ALS(userCol=\"_3\",itemCol=\"_1\",ratingCol=\"_2\",coldStartStrategy=\"drop\")\n",
    "\n",
    "parameters=ParamGridBuilder().build()\n",
    "eval = RegressionEvaluator(metricName= \"rmse\",labelCol= \"_2\",predictionCol= \"prediction\")\n",
    "\n",
    "#Build train validation split\n",
    "trainvs = TrainValidationSplit(estimator=als,estimatorParamMaps=parameters,evaluator=eval)\n",
    "#Build cross validator\n",
    "cv = CrossValidator(estimator =als,estimatorParamMaps =parameters,evaluator =eval,numFolds=3)\n",
    "\n",
    "#Fit model to training data\n",
    "model = trainvs.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "rmse = RegressionEvaluator(metricName= \"rmse\",labelCol= \"_2\",predictionCol= \"prediction\").evaluate(predictions)\n",
    "print(\"RMSE=\",rmse)\n",
    "mse = RegressionEvaluator(metricName= \"mse\",labelCol= \"_2\",predictionCol= \"prediction\").evaluate(predictions)\n",
    "print(\"MSE=\",mse)\n",
    "mae = RegressionEvaluator(metricName= \"mae\",labelCol= \"_2\",predictionCol= \"prediction\").evaluate(predictions)\n",
    "print(\"MAE=\",mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6b8f6d-6d3e-4b79-9fe4-701d803d0448",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE= 1.2104203541555707\nMSE= 1.4651174337540973\nMAE= 0.7975462714062076\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator,TrainValidationSplit\n",
    "spark=SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "RDD = spark.sparkContext.textFile(\"/FileStore/tables/movies.csv\").filter(lambda x:x!=\"movieId,rating,userId\")\n",
    "df=RDD.map(lambda line: (int(line.split(\",\")[0]),int(line.split(\",\")[1]),int(line.split(\",\")[2]))).toDF()\n",
    "train, test = df.randomSplit(weights=[0.7, 0.3],seed=2)\n",
    "als=ALS(userCol=\"_3\",itemCol=\"_1\",ratingCol=\"_2\",coldStartStrategy=\"drop\")\n",
    "\n",
    "parameters=ParamGridBuilder()\\\n",
    ".addGrid(als.rank,[16])\\\n",
    ".addGrid(als.maxIter, [10, 20])\\\n",
    ".addGrid(als.regParam, [0.05, 0.1])\\\n",
    ".addGrid(als.numItemBlocks, [5, 10])\\\n",
    ".addGrid(als.numUserBlocks, [5, 10]).build()\n",
    "\n",
    "eval = RegressionEvaluator(metricName= \"rmse\",labelCol= \"_2\",predictionCol= \"prediction\")\n",
    "\n",
    "#Build train validation split\n",
    "trainvs = TrainValidationSplit(estimator=als,estimatorParamMaps=parameters,evaluator=eval)\n",
    "#Build cross validator\n",
    "cv = CrossValidator(estimator =als,estimatorParamMaps =parameters,evaluator =eval,numFolds=3)\n",
    "\n",
    "#Fit model to training data\n",
    "model = trainvs.fit(train)\n",
    "predictions = model.transform(test)\n",
    "#print(model.bestModel.rank)\n",
    "\n",
    "rmse = RegressionEvaluator(metricName= \"rmse\",labelCol= \"_2\",predictionCol= \"prediction\").evaluate(predictions)\n",
    "print(\"RMSE=\",rmse)\n",
    "mse = RegressionEvaluator(metricName= \"mse\",labelCol= \"_2\",predictionCol= \"prediction\").evaluate(predictions)\n",
    "print(\"MSE=\",mse)\n",
    "mae = RegressionEvaluator(metricName= \"mae\",labelCol= \"_2\",predictionCol= \"prediction\").evaluate(predictions)\n",
    "print(\"MAE=\",mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512fb9b2-621c-46d0-a72b-8bcd627a798b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(model.bestModel.rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a7735e-5882-49f7-8887-0cb6edf5f0ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the top 15 movie recommendations for userId 10 is:\n+---+---+---+----------+\n| _1| _2| _3|prediction|\n+---+---+---+----------+\n|  2| -1| 10|  3.150111|\n| 42| -1| 10| 2.6256824|\n| 53| -1| 10|  2.383279|\n| 74| -1| 10| 1.8984079|\n| 47| -1| 10| 1.8643752|\n| 16| -1| 10| 1.8482214|\n| 29| -1| 10| 1.7579231|\n| 13| -1| 10| 1.7041345|\n| 95| -1| 10| 1.6817774|\n| 52| -1| 10| 1.6714218|\n| 67| -1| 10| 1.6698463|\n| 49| -1| 10| 1.6678468|\n| 62| -1| 10| 1.6515498|\n| 85| -1| 10| 1.6494863|\n|  7| -1| 10| 1.5484833|\n+---+---+---+----------+\nonly showing top 15 rows\n\nthe top 15 movie recommendations for userId 14 is:\n+---+---+---+----------+\n| _1| _2| _3|prediction|\n+---+---+---+----------+\n| 29| -1| 14| 4.8037715|\n| 52| -1| 14| 4.5231886|\n| 76| -1| 14| 4.2602873|\n| 63| -1| 14|  4.132693|\n| 62| -1| 14| 3.7405062|\n| 72| -1| 14| 3.2885995|\n| 70| -1| 14| 3.0416458|\n| 95| -1| 14| 2.9015746|\n| 31| -1| 14| 2.8322818|\n| 47| -1| 14|  2.831221|\n| 58| -1| 14| 2.7945845|\n| 69| -1| 14| 2.7755332|\n|  2| -1| 14|  2.749929|\n| 85| -1| 14|  2.677878|\n| 14| -1| 14| 2.6541696|\n+---+---+---+----------+\nonly showing top 15 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator,TrainValidationSplit\n",
    "spark=SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "df10 = [(i,-1, 10) for i in range(1, 101)]\n",
    "df10 = spark.createDataFrame(df10, schema=[\"_1\", \"_2\",\"_3\"])\n",
    "rec10=model.transform(df10)\n",
    "print(\"the top 15 movie recommendations for userId 10 is:\")\n",
    "rec10.sort(\"prediction\",ascending=False).show(15)\n",
    "\n",
    "df14 = [(i,-1, 14) for i in range(1, 101)]\n",
    "df14 = spark.createDataFrame(df14, schema=[\"_1\", \"_2\",\"_3\"])\n",
    "rec14=model.transform(df14)\n",
    "print(\"the top 15 movie recommendations for userId 14 is:\")\n",
    "rec14.sort(\"prediction\",ascending=False).show(15)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1628A2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
